{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>label</th>\n",
       "      <th>path</th>\n",
       "      <th>transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>daningram_15</td>\n",
       "      <td>dementia</td>\n",
       "      <td>data/dementia/Dan Ingram/daningram_15.wav</td>\n",
       "      <td>Long before the blackout. All right, yeah. Bla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>terryjones_5</td>\n",
       "      <td>dementia</td>\n",
       "      <td>data/dementia/Terry Jones/terryjones_5.wav</td>\n",
       "      <td>Well, yeah, but then it was the government tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>maureenforrester_5</td>\n",
       "      <td>dementia</td>\n",
       "      <td>data/dementia/Maureen Forrester/maureenforrest...</td>\n",
       "      <td>You know, if you've ever gone to a place, a ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aileenhernandez_0</td>\n",
       "      <td>dementia</td>\n",
       "      <td>data/dementia/Aileen Hernandez/aileenhernandez...</td>\n",
       "      <td>This is not going to sound like very ladylike....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aileenhernandez_5_1</td>\n",
       "      <td>dementia</td>\n",
       "      <td>data/dementia/Aileen Hernandez/aileenhernandez...</td>\n",
       "      <td>I arrive at my first political science class, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Angela Lansbury_1</td>\n",
       "      <td>nodementia</td>\n",
       "      <td>data/nodementia/Angela Lansbury/Angela Lansbur...</td>\n",
       "      <td>Of course, I'd always been associated with mov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>Angela Lansbury_3</td>\n",
       "      <td>nodementia</td>\n",
       "      <td>data/nodementia/Angela Lansbury/Angela Lansbur...</td>\n",
       "      <td>I only really come alive as an interesting per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>Angela Lansbury_2</td>\n",
       "      <td>nodementia</td>\n",
       "      <td>data/nodementia/Angela Lansbury/Angela Lansbur...</td>\n",
       "      <td>Opened the door to a solution in my mind. Our ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>BobNewhart_2</td>\n",
       "      <td>nodementia</td>\n",
       "      <td>data/nodementia/Bob Newhart/BobNewhart_2.wav</td>\n",
       "      <td>That's a tough. I don't know. When Bob wasn't ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>BobNewhart_3</td>\n",
       "      <td>nodementia</td>\n",
       "      <td>data/nodementia/Bob Newhart/BobNewhart_3.wav</td>\n",
       "      <td>Here is the, you know, JFK once said, victory ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    file       label  \\\n",
       "0           daningram_15    dementia   \n",
       "1           terryjones_5    dementia   \n",
       "2     maureenforrester_5    dementia   \n",
       "3      aileenhernandez_0    dementia   \n",
       "4    aileenhernandez_5_1    dementia   \n",
       "..                   ...         ...   \n",
       "150    Angela Lansbury_1  nodementia   \n",
       "151    Angela Lansbury_3  nodementia   \n",
       "152    Angela Lansbury_2  nodementia   \n",
       "225         BobNewhart_2  nodementia   \n",
       "226         BobNewhart_3  nodementia   \n",
       "\n",
       "                                                  path  \\\n",
       "0            data/dementia/Dan Ingram/daningram_15.wav   \n",
       "1           data/dementia/Terry Jones/terryjones_5.wav   \n",
       "2    data/dementia/Maureen Forrester/maureenforrest...   \n",
       "3    data/dementia/Aileen Hernandez/aileenhernandez...   \n",
       "4    data/dementia/Aileen Hernandez/aileenhernandez...   \n",
       "..                                                 ...   \n",
       "150  data/nodementia/Angela Lansbury/Angela Lansbur...   \n",
       "151  data/nodementia/Angela Lansbury/Angela Lansbur...   \n",
       "152  data/nodementia/Angela Lansbury/Angela Lansbur...   \n",
       "225       data/nodementia/Bob Newhart/BobNewhart_2.wav   \n",
       "226       data/nodementia/Bob Newhart/BobNewhart_3.wav   \n",
       "\n",
       "                                         transcription  \n",
       "0    Long before the blackout. All right, yeah. Bla...  \n",
       "1    Well, yeah, but then it was the government tha...  \n",
       "2    You know, if you've ever gone to a place, a ho...  \n",
       "3    This is not going to sound like very ladylike....  \n",
       "4    I arrive at my first political science class, ...  \n",
       "..                                                 ...  \n",
       "150  Of course, I'd always been associated with mov...  \n",
       "151  I only really come alive as an interesting per...  \n",
       "152  Opened the door to a solution in my mind. Our ...  \n",
       "225  That's a tough. I don't know. When Bob wasn't ...  \n",
       "226  Here is the, you know, JFK once said, victory ...  \n",
       "\n",
       "[124 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data\\\\train_dm_transcription.csv\" , header = \"infer\")\n",
    "# dropping rows with null values \n",
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text', '__index_level_0__'],\n",
       "        num_rows: 99\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text', '__index_level_0__'],\n",
       "        num_rows: 25\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['label' , 'transcription']]\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.rename_column('transcription', 'text')\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "train_dataset = train_dataset.rename_column('transcription', 'text')\n",
    "val_dataset = val_dataset.rename_column('transcription', 'text')\n",
    "\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': val_dataset\n",
    "})\n",
    "\n",
    "dataset_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sashr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "def tokenize(example):\n",
    "    text = example.get('text', \"\")\n",
    "    label = example.get('label')  \n",
    "    \n",
    "    if text is None:\n",
    "        text = \"\"  \n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text,                         \n",
    "        add_special_tokens=True,      \n",
    "        max_length=128,                \n",
    "        padding='max_length',         \n",
    "        truncation=True,              \n",
    "        return_tensors='pt'          \n",
    "    )\n",
    "    \n",
    "    input_ids = encoded_dict['input_ids'].squeeze(0)\n",
    "    attention_mask = encoded_dict['attention_mask'].squeeze(0)\n",
    "    label_int = 1\n",
    "\n",
    "    if label=='dementia':\n",
    "        label_int = 0\n",
    "    if label=='nodementia':\n",
    "        label_int = 1 \n",
    "     \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'label': label_int\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f582f020ae4024832884db7976f1e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/99 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebad5af99e634716b7d60d3796883011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_dict = dataset_dict.map(tokenize, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn as nn \n",
    "Bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class BertModel(nn.Module):  \n",
    "    def __init__(self, bert_layer, num_classes):\n",
    "        super(BertModel, self).__init__()  \n",
    "        self.bert = bert_layer\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)  \n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x = outputs.pooler_output  \n",
    "        x = self.fc(x)\n",
    "        x = nn.Softmax(dim=1)(x)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = BertModel(Bert_layer , num_classes=4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inps = []\n",
    "atts = []\n",
    "lab = []\n",
    "i = 0\n",
    "for data in dataset_dict['train']:\n",
    "    if i > 10000 : \n",
    "        break \n",
    "    i+=1\n",
    "    inp = data['input_ids']\n",
    "    inp = torch.tensor(inp).unsqueeze(0) \n",
    "    inp = inp.squeeze(1)\n",
    "    att = data['attention_mask']\n",
    "    att = torch.tensor(att).unsqueeze(0) \n",
    "    att = att.squeeze(1)\n",
    "    inps.append(inp)\n",
    "    atts.append(att)\n",
    "    lab.append(data['label'])\n",
    "\n",
    "inps = torch.stack(inps)\n",
    "atts = torch.stack(atts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([99, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([99, 128])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inps = inps.squeeze(1)\n",
    "print(inps.shape)\n",
    "atts = atts.squeeze(1)\n",
    "atts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 10/10 [02:09<00:00, 12.94s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.1296, Accuracy: 0.6364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 10/10 [03:35<00:00, 21.53s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Loss: 0.1133, Accuracy: 0.7475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 10/10 [02:08<00:00, 12.88s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Loss: 0.1074, Accuracy: 0.7475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 10/10 [03:07<00:00, 18.79s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Loss: 0.1041, Accuracy: 0.7475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 10/10 [01:55<00:00, 11.53s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Loss: 0.1027, Accuracy: 0.7475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 10/10 [01:56<00:00, 11.68s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Loss: 0.1020, Accuracy: 0.7475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 10/10 [02:07<00:00, 12.75s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Loss: 0.1018, Accuracy: 0.7475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 10/10 [01:55<00:00, 11.51s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Loss: 0.1013, Accuracy: 0.7475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 10/10 [03:38<00:00, 21.86s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Loss: 0.1013, Accuracy: 0.7475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 10/10 [01:57<00:00, 11.80s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Loss: 0.1013, Accuracy: 0.7475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from tqdm import tqdm  \n",
    "labels = torch.tensor(lab)\n",
    "dataset = TensorDataset(inps, atts, labels)\n",
    "data_loader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "for epoch in range(10):\n",
    "    classifier.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    for batch in tqdm(data_loader, desc=f\"Epoch {epoch+1}\", unit=\"batch\"):\n",
    "        input_ids, attention_mask, labels = batch  \n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier(input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct_predictions+=(preds == labels).sum().item()\n",
    "    avg_loss = total_loss/len(dataset)\n",
    "    accuracy = correct_predictions/len(dataset)\n",
    "    print(f\"Epoch {epoch+1} - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 10/10 [00:40<00:00,  4.09s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing - Loss: 0.1013, Accuracy: 0.7475\n"
     ]
    }
   ],
   "source": [
    "# testing the model \n",
    "classifier.eval()\n",
    "total_loss = 0\n",
    "correct_predictions = 0\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(data_loader, desc=\"Testing\", unit=\"batch\"):\n",
    "        input_ids, attention_mask, labels = batch  \n",
    "        outputs = classifier(input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct_predictions+=(preds == labels).sum().item()\n",
    "    avg_loss = total_loss/len(dataset)\n",
    "    accuracy = correct_predictions/len(dataset)\n",
    "    print(f\"Testing - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "# saving the model\n",
    "torch.save(classifier.state_dict(), \"bert_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
